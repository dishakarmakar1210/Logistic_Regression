{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "--> Logistic Regression is a classification algorithm that predicts the probability of a binary (or multiclass, via extensions) outcome, mapping inputs to values between 0 and 1. Instead of fitting a straight line to y, it fits an S‑shaped curve using the sigmoid (logistic) function. Linear Regression, by contrast, predicts a continuous numeric target and minimizes squared error, whereas Logistic Regression optimizes a likelihood-based cost for categorical decisions.\n",
        "\n",
        "---\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "--> For a single instance with feature vector x, the model computes\n",
        "\n",
        "𝑝\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        ",\n",
        "where\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑘\n",
        "𝑥\n",
        "𝑘\n",
        "p(y=1∣x)=σ(z)=\n",
        "1+e\n",
        "−z\n",
        "1\n",
        "​\n",
        " ,where z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "k\n",
        "​\n",
        " x\n",
        "k\n",
        "​\n",
        "\n",
        "Here σ( ) is the sigmoid function, β are the learned coefficients, and the output p is the estimated probability of the positive class.\n",
        "\n",
        "---\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "--> The sigmoid squashes any real‑valued input to the open interval (0, 1), letting the linear combination of features be interpreted as a probability. Its smooth, differentiable shape enables gradient‑based optimization, and its log‑odds interpretation (\n",
        "log\n",
        "⁡\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "=\n",
        "𝑧\n",
        "log\n",
        "1−p\n",
        "p\n",
        "​\n",
        " =z) makes coefficients easy to read as log‑odds changes.\n",
        "\n",
        "---\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "--> Logistic Regression minimizes the negative log‑likelihood, often called binary cross‑entropy:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "𝑝\n",
        "𝑖\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "J(β)=−\n",
        "n\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " logp\n",
        "i\n",
        "​\n",
        " +(1−y\n",
        "i\n",
        "​\n",
        " )log(1−p\n",
        "i\n",
        "​\n",
        " )]\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        "𝑖\n",
        ")\n",
        "p\n",
        "i\n",
        "​\n",
        " =σ(z\n",
        "i\n",
        "​\n",
        " ). Minimizing this convex loss finds the parameter set that maximizes the probability of observing the training labels.\n",
        "\n",
        "---\n",
        "\n",
        "5. What is Regularization in Logistic Regression and why is it needed?\n",
        "--> Regularization adds a penalty term to the cost function to discourage overly large coefficients, thereby reducing variance and mitigating overfitting. By shrinking or constraining β, the model becomes simpler, more stable on unseen data, and less sensitive to multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "--> Ridge (L2) adds\n",
        "𝜆\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "λ∑β\n",
        "j\n",
        "2\n",
        "​\n",
        " , shrinking coefficients smoothly toward zero without eliminating any.\n",
        "Lasso (L1) adds\n",
        "𝜆\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "λ∑∣β\n",
        "j\n",
        "​\n",
        " ∣, which can drive some coefficients exactly to zero, performing built‑in feature selection.\n",
        "Elastic Net blends both penalties:\n",
        "𝜆\n",
        "[\n",
        "𝛼\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "]\n",
        "λ[α∑∣β\n",
        "j\n",
        "​\n",
        " ∣+(1−α)∑β\n",
        "j\n",
        "2\n",
        "​\n",
        " ], balancing Ridge’s stability with Lasso’s sparsity.\n",
        "\n",
        "---\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "--> Elastic Net excels when you have many correlated predictors or expect only a subset to be truly influential. Ridge alone can’t drop redundant features, and Lasso may arbitrarily pick one among correlated variables; Elastic Net tends to share weights across correlated groups while still allowing others to shrink to zero, yielding a more reliable, interpretable model.\n",
        "\n",
        "---\n",
        "\n",
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "--> λ controls the strength of the penalty: a larger λ enforces more shrinkage, increasing bias but lowering variance (risking underfitting), while a smaller λ allows coefficients to grow, reducing bias but heightening variance (risking overfitting). Model performance typically peaks at an intermediate λ found via cross‑validation.\n",
        "\n",
        "---\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "--> The true log‑odds of the outcome is linearly related to the predictors.\n",
        "\n",
        "Observations are independent.\n",
        "\n",
        "There is little or no multicollinearity among predictors.\n",
        "\n",
        "The sample is sufficiently large to approximate maximum‑likelihood properties. Unlike linear regression, homoscedasticity and normal residuals are not required.\n",
        "\n",
        "---\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "--> Common alternatives include Decision Trees, Random Forests, Gradient Boosting (e.g., XGBoost, LightGBM), Support Vector Machines, k‑Nearest Neighbors, Naïve Bayes, and neural networks (from simple MLPs to deep architectures like CNNs or Transformers). Choice depends on data size, feature types, interpretability needs, and non‑linearity of relationships.\n",
        "\n",
        "---\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "--> Classification metrics evaluate how well a model distinguishes between classes. Key metrics include Accuracy, Precision, Recall, F1-Score, and ROC-AUC. While accuracy is easy to understand, F1-score balances precision and recall, making it useful in imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "--> Class imbalance can lead to biased predictions toward the majority class, reducing the model’s ability to detect the minority class. This inflates accuracy while harming precision, recall, and F1-score for the minority. To address this, techniques like class weighting, resampling, or anomaly detection methods are used.\n",
        "\n",
        "---\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "--> Hyperparameter tuning involves selecting optimal values for parameters like the regularization strength (C) and penalty type (L1/L2). It’s typically done using grid search or randomized search with cross-validation. Tuning improves model generalization by preventing underfitting or overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "--> Common solvers include liblinear, saga, newton-cg, lbfgs, and sag.\n",
        "\n",
        "liblinear is good for small datasets and L1 regularization.\n",
        "\n",
        "saga handles large datasets and supports both L1 and L2.\n",
        "\n",
        "lbfgs is efficient for multiclass and dense data.\n",
        "Choose based on data size, sparsity, and regularization.\n",
        "\n",
        "---\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "--> Multiclass Logistic Regression can be extended using One-vs-Rest (OvR) or Softmax (Multinomial) strategies. OvR trains one binary classifier per class, while Softmax generalizes logistic regression by estimating probabilities across all classes simultaneously. Most libraries (like scikit-learn) support both.\n",
        "\n",
        "---\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "--> Advantages: It’s easy to implement, interpretable, efficient on small datasets, and works well with linearly separable data.\n",
        "Disadvantages: It struggles with non-linear patterns, sensitive to outliers, assumes linearity in log-odds, and doesn’t perform well with complex feature interactions unless explicitly modeled.\n",
        "\n",
        "---\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        "--> Logistic Regression is widely used in spam detection, credit scoring, disease diagnosis (e.g., diabetes prediction), customer churn prediction, and ad click prediction. Its interpretability makes it popular in regulated industries like healthcare and finance.\n",
        "\n",
        "---\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "--> Logistic Regression is binary and uses the sigmoid function to model two-class problems. Softmax Regression (Multinomial Logistic Regression) generalizes this to multiple classes using the softmax function, which ensures predicted class probabilities sum to 1 across all classes.\n",
        "\n",
        "---\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "--> OvR is simpler, trains multiple binary models, and works well when classes are well-separated. Softmax handles all classes in one model and is better when the classes are interrelated or overlapping. For large, balanced datasets with interconnected classes, Softmax is usually preferred.\n",
        "\n",
        "---\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "--> Each coefficient represents the change in the log-odds of the target class for a one-unit increase in the predictor, keeping other variables constant. A positive coefficient increases the probability of the positive class; a negative one decreases it. Exponentiating the coefficient gives the odds ratio, useful for interpretation."
      ],
      "metadata": {
        "id": "8VT4nniG5k0V"
      }
    }
  ]
}