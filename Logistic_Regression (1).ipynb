{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "--> Logistic Regression is a classification algorithm that predicts the probability of a binary (or multiclass, via extensions) outcome, mapping inputs to values between 0 and 1. Instead of fitting a straight line to y, it fits an S‑shaped curve using the sigmoid (logistic) function. Linear Regression, by contrast, predicts a continuous numeric target and minimizes squared error, whereas Logistic Regression optimizes a likelihood-based cost for categorical decisions.\n",
        "\n",
        "---\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "--> For a single instance with feature vector x, the model computes\n",
        "\n",
        "𝑝\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        ",\n",
        "where\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑘\n",
        "𝑥\n",
        "𝑘\n",
        "p(y=1∣x)=σ(z)=\n",
        "1+e\n",
        "−z\n",
        "1\n",
        "​\n",
        " ,where z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "k\n",
        "​\n",
        " x\n",
        "k\n",
        "​\n",
        "\n",
        "Here σ( ) is the sigmoid function, β are the learned coefficients, and the output p is the estimated probability of the positive class.\n",
        "\n",
        "---\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "--> The sigmoid squashes any real‑valued input to the open interval (0, 1), letting the linear combination of features be interpreted as a probability. Its smooth, differentiable shape enables gradient‑based optimization, and its log‑odds interpretation (\n",
        "log\n",
        "⁡\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "=\n",
        "𝑧\n",
        "log\n",
        "1−p\n",
        "p\n",
        "​\n",
        " =z) makes coefficients easy to read as log‑odds changes.\n",
        "\n",
        "---\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "--> Logistic Regression minimizes the negative log‑likelihood, often called binary cross‑entropy:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "𝑝\n",
        "𝑖\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "J(β)=−\n",
        "n\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " logp\n",
        "i\n",
        "​\n",
        " +(1−y\n",
        "i\n",
        "​\n",
        " )log(1−p\n",
        "i\n",
        "​\n",
        " )]\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        "𝑖\n",
        ")\n",
        "p\n",
        "i\n",
        "​\n",
        " =σ(z\n",
        "i\n",
        "​\n",
        " ). Minimizing this convex loss finds the parameter set that maximizes the probability of observing the training labels.\n",
        "\n",
        "---\n",
        "\n",
        "5. What is Regularization in Logistic Regression and why is it needed?\n",
        "--> Regularization adds a penalty term to the cost function to discourage overly large coefficients, thereby reducing variance and mitigating overfitting. By shrinking or constraining β, the model becomes simpler, more stable on unseen data, and less sensitive to multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "--> Ridge (L2) adds\n",
        "𝜆\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "λ∑β\n",
        "j\n",
        "2\n",
        "​\n",
        " , shrinking coefficients smoothly toward zero without eliminating any.\n",
        "Lasso (L1) adds\n",
        "𝜆\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "λ∑∣β\n",
        "j\n",
        "​\n",
        " ∣, which can drive some coefficients exactly to zero, performing built‑in feature selection.\n",
        "Elastic Net blends both penalties:\n",
        "𝜆\n",
        "[\n",
        "𝛼\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "]\n",
        "λ[α∑∣β\n",
        "j\n",
        "​\n",
        " ∣+(1−α)∑β\n",
        "j\n",
        "2\n",
        "​\n",
        " ], balancing Ridge’s stability with Lasso’s sparsity.\n",
        "\n",
        "---\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "--> Elastic Net excels when you have many correlated predictors or expect only a subset to be truly influential. Ridge alone can’t drop redundant features, and Lasso may arbitrarily pick one among correlated variables; Elastic Net tends to share weights across correlated groups while still allowing others to shrink to zero, yielding a more reliable, interpretable model.\n",
        "\n",
        "---\n",
        "\n",
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "--> λ controls the strength of the penalty: a larger λ enforces more shrinkage, increasing bias but lowering variance (risking underfitting), while a smaller λ allows coefficients to grow, reducing bias but heightening variance (risking overfitting). Model performance typically peaks at an intermediate λ found via cross‑validation.\n",
        "\n",
        "---\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "--> The true log‑odds of the outcome is linearly related to the predictors.\n",
        "\n",
        "Observations are independent.\n",
        "\n",
        "There is little or no multicollinearity among predictors.\n",
        "\n",
        "The sample is sufficiently large to approximate maximum‑likelihood properties. Unlike linear regression, homoscedasticity and normal residuals are not required.\n",
        "\n",
        "---\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "--> Common alternatives include Decision Trees, Random Forests, Gradient Boosting (e.g., XGBoost, LightGBM), Support Vector Machines, k‑Nearest Neighbors, Naïve Bayes, and neural networks (from simple MLPs to deep architectures like CNNs or Transformers). Choice depends on data size, feature types, interpretability needs, and non‑linearity of relationships.\n",
        "\n",
        "---\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "--> Classification metrics evaluate how well a model distinguishes between classes. Key metrics include Accuracy, Precision, Recall, F1-Score, and ROC-AUC. While accuracy is easy to understand, F1-score balances precision and recall, making it useful in imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "--> Class imbalance can lead to biased predictions toward the majority class, reducing the model’s ability to detect the minority class. This inflates accuracy while harming precision, recall, and F1-score for the minority. To address this, techniques like class weighting, resampling, or anomaly detection methods are used.\n",
        "\n",
        "---\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "--> Hyperparameter tuning involves selecting optimal values for parameters like the regularization strength (C) and penalty type (L1/L2). It’s typically done using grid search or randomized search with cross-validation. Tuning improves model generalization by preventing underfitting or overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "--> Common solvers include liblinear, saga, newton-cg, lbfgs, and sag.\n",
        "\n",
        "liblinear is good for small datasets and L1 regularization.\n",
        "\n",
        "saga handles large datasets and supports both L1 and L2.\n",
        "\n",
        "lbfgs is efficient for multiclass and dense data.\n",
        "Choose based on data size, sparsity, and regularization.\n",
        "\n",
        "---\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "--> Multiclass Logistic Regression can be extended using One-vs-Rest (OvR) or Softmax (Multinomial) strategies. OvR trains one binary classifier per class, while Softmax generalizes logistic regression by estimating probabilities across all classes simultaneously. Most libraries (like scikit-learn) support both.\n",
        "\n",
        "---\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "--> Advantages: It’s easy to implement, interpretable, efficient on small datasets, and works well with linearly separable data.\n",
        "Disadvantages: It struggles with non-linear patterns, sensitive to outliers, assumes linearity in log-odds, and doesn’t perform well with complex feature interactions unless explicitly modeled.\n",
        "\n",
        "---\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        "--> Logistic Regression is widely used in spam detection, credit scoring, disease diagnosis (e.g., diabetes prediction), customer churn prediction, and ad click prediction. Its interpretability makes it popular in regulated industries like healthcare and finance.\n",
        "\n",
        "---\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "--> Logistic Regression is binary and uses the sigmoid function to model two-class problems. Softmax Regression (Multinomial Logistic Regression) generalizes this to multiple classes using the softmax function, which ensures predicted class probabilities sum to 1 across all classes.\n",
        "\n",
        "---\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "--> OvR is simpler, trains multiple binary models, and works well when classes are well-separated. Softmax handles all classes in one model and is better when the classes are interrelated or overlapping. For large, balanced datasets with interconnected classes, Softmax is usually preferred.\n",
        "\n",
        "---\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "--> Each coefficient represents the change in the log-odds of the target class for a one-unit increase in the predictor, keeping other variables constant. A positive coefficient increases the probability of the positive class; a negative one decreases it. Exponentiating the coefficient gives the odds ratio, useful for interpretation."
      ],
      "metadata": {
        "id": "8VT4nniG5k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "csv_path = \"dataset.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "X = df.drop(columns=\"target\")\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Basic accuracy:\", accuracy_score(y_te, model.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWdotXsw-XGz",
        "outputId": "fb22a543-860e-4c2b-b7a1-8bbf2f8616c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic accuracy: 0.9813333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "l1_clf = LogisticRegression(penalty=\"l1\", solver=\"saga\", C=1.0, max_iter=5000)\n",
        "l1_clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"L1 accuracy:\", accuracy_score(y_te, l1_clf.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j59TPszKGYUw",
        "outputId": "0e82a279-fdde-41fa-cf6d-fe53f35cdb66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 accuracy: 0.9813333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split( X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "l2_clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=1000)\n",
        "l2_clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"L2 accuracy :\", accuracy_score(y_te, l2_clf.predict(X_te)))\n",
        "print(\"First‑10 coeffs of class 0:\", np.round(l2_clf.coef_[0][:10], 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXXJciAgHJ7J",
        "outputId": "1d160911-9849-47db-b4db-11aaf0c5d013"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 accuracy : 0.9813333333333333\n",
            "First‑10 coeffs of class 0: [ 0.077 -0.007 -0.185 -0.096 -0.199 -0.003 -0.23  -0.012  0.367  0.185]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "en_clf = LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5, C=1.0, max_iter=5000)\n",
        "en_clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Elastic‑Net accuracy:\", accuracy_score(y_te, en_clf.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-7ACGfeHahb",
        "outputId": "1e163be1-d808-4a77-aa22-faf440bdcbb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic‑Net accuracy: 0.984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=0)\n",
        "\n",
        "ovr_clf = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=1000)\n",
        "ovr_clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"OvR accuracy:\", accuracy_score(y_te, ovr_clf.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2FB4UnnHn0x",
        "outputId": "89659e02-ff0a-41d3-e162-ec758261054e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR accuracy: 0.9911111111111112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "param_grid = [\n",
        "    {\"penalty\": [\"l1\"], \"solver\": [\"saga\"], \"C\": [0.01, 0.1, 1, 10]},\n",
        "    {\"penalty\": [\"l2\"], \"solver\": [\"lbfgs\"], \"C\": [0.01, 0.1, 1, 10]},\n",
        "    {\"penalty\": [\"elasticnet\"], \"solver\": [\"saga\"], \"l1_ratio\": [0.3, 0.5], \"C\": [0.1, 1, 10]}\n",
        "]\n",
        "\n",
        "search = GridSearchCV(LogisticRegression(max_iter=5000, multi_class=\"multinomial\"), param_grid, cv=5, verbose = 0, scoring=\"accuracy\")\n",
        "search.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Best CV accuracy:\", round(search.best_score_, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQx_VuHtH8t1",
        "outputId": "0c584b84-ee0c-4a3a-a8b1-e74277127990"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'C': 0.1, 'l1_ratio': 0.3, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Best CV accuracy: 0.982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "scores = cross_val_score(LogisticRegression(max_iter=1000),X, y, cv=skf, scoring=\"accuracy\")\n",
        "\n",
        "print(\"Fold accuracies:\", [f\"{s:.3f}\" for s in scores])\n",
        "print(\"Average accuracy:\", round(scores.mean(), 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNgrqLUWIymM",
        "outputId": "de8aa553-1af6-41b4-e598-4daefdc1bc69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold accuracies: ['0.970', '0.977', '0.987', '0.987', '0.983']\n",
            "Average accuracy: 0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"CSV accuracy:\", accuracy_score(y_te, clf.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGrVj_9kJEgJ",
        "outputId": "0083238a-6833-4f8e-d3b5-3499d2be56f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "import pandas as pd\n",
        "from scipy.stats import loguniform\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "# Parameter space — all combos here are valid for multinomial problems\n",
        "param_dists = [\n",
        "    {\"solver\": [\"lbfgs\"], \"penalty\": [\"l2\"], \"C\": loguniform(1e-3, 1e2)},\n",
        "    {\"solver\": [\"saga\"],  \"penalty\": [\"l1\", \"l2\"], \"C\": loguniform(1e-3, 1e2)}\n",
        "]\n",
        "\n",
        "search = RandomizedSearchCV(LogisticRegression(max_iter=5000, multi_class=\"multinomial\"),param_dists, n_iter=20, cv=5, scoring=\"accuracy\", random_state=0)\n",
        "search.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Best parameters :\", search.best_params_)\n",
        "print(\"Best CV accuracy:\", round(search.best_score_, 3))\n",
        "print(\"Test‑set accuracy:\", round(accuracy_score(y_te, search.best_estimator_.predict(X_te)), 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewb8Jf6TJVLU",
        "outputId": "74e5f98f-c12b-44e1-9ba3-e3dc5d6eee2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters : {'C': np.float64(0.08354270566129816), 'penalty': 'l2', 'solver': 'saga'}\n",
            "Best CV accuracy: 0.982\n",
            "Test‑set accuracy: 0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=0)\n",
        "\n",
        "ovo = OneVsOneClassifier(LogisticRegression(max_iter=1000, solver=\"lbfgs\"))\n",
        "ovo.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"OvO accuracy:\", accuracy_score(y_te, ovo.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o304FupyKWeL",
        "outputId": "46010cde-4a55-4267-9a53-68f3b78d7b68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO accuracy: 0.9866666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), (df[\"target\"] == 0).astype(int)  # binary\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "cm = confusion_matrix(y_te, clf.predict(X_te))\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Other\", \"Class 0\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "azYCAwQzKkZw",
        "outputId": "02ccef98-5322-496b-bcf1-de0244360e81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHHCAYAAABtF1i4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAR0hJREFUeJzt3XlYVeX6//HPBmWDyODIoIg4Y45px0OWw1cUxzRtsKiwHE6FmbN1yrHM81XL0kw71tfpaFqntKOVQ1pSSaalZWqmOBaiJiKCMgjr94c/9mkLKls2ro37/epa18Ve61nPute+VO7u53nWshiGYQgAAMAkHmYHAAAA3BvJCAAAMBXJCAAAMBXJCAAAMBXJCAAAMBXJCAAAMBXJCAAAMBXJCAAAMBXJCAAAMBXJCFAGHThwQF26dFFAQIAsFotWr17t1P6PHDkii8WiRYsWObXfsqxDhw7q0KGD2WEAtySSEeAGJSUl6W9/+5vq1Kkjb29v+fv7q23btnrjjTd08eLFUr12XFycdu/eralTp2rp0qVq3bp1qV7vZhowYIAsFov8/f2L/B4PHDggi8Uii8WimTNnOtx/cnKyJk2apF27djkhWgDOUM7sAICy6JNPPtH9998vq9Wqxx57TE2aNFFOTo6+/vprjRkzRnv27NE///nPUrn2xYsXlZiYqBdeeEFDhw4tlWuEh4fr4sWLKl++fKn0fz3lypXThQsXtGbNGj3wwAN2x5YtWyZvb29lZWXdUN/JycmaPHmyateurRYtWhT7vA0bNtzQ9QBcH8kI4KDDhw+rf//+Cg8P1+bNmxUSEmI7Fh8fr4MHD+qTTz4pteufPn1akhQYGFhq17BYLPL29i61/q/HarWqbdu2eu+99wolI8uXL1ePHj304Ycf3pRYLly4oAoVKsjLy+umXA9wRwzTAA6aPn26MjIy9O6779olIgXq1aunZ5991vb50qVLeumll1S3bl1ZrVbVrl1bf//735WdnW13Xu3atdWzZ099/fXX+stf/iJvb2/VqVNHS5YssbWZNGmSwsPDJUljxoyRxWJR7dq1JV0e3ij4+c8mTZoki8Vit2/jxo266667FBgYqIoVK6phw4b6+9//bjt+tTkjmzdv1t133y1fX18FBgaqd+/e2rdvX5HXO3jwoAYMGKDAwEAFBATo8ccf14ULF67+xV7h4Ycf1meffaa0tDTbvu3bt+vAgQN6+OGHC7VPTU3V6NGj1bRpU1WsWFH+/v7q1q2bfvzxR1ubL7/8UnfccYck6fHHH7cN9xTcZ4cOHdSkSRN9//33ateunSpUqGD7Xq6cMxIXFydvb+9C9x8TE6NKlSopOTm52PcKuDuSEcBBa9asUZ06dXTnnXcWq/2gQYM0YcIE3X777Zo1a5bat2+vadOmqX///oXaHjx4UPfdd586d+6sV199VZUqVdKAAQO0Z88eSVLfvn01a9YsSdJDDz2kpUuX6vXXX3co/j179qhnz57Kzs7WlClT9Oqrr+qee+7RN998c83zPv/8c8XExOjUqVOaNGmSRo4cqa1bt6pt27Y6cuRIofYPPPCAzp8/r2nTpumBBx7QokWLNHny5GLH2bdvX1ksFn300Ue2fcuXL1ejRo10++23F2p/6NAhrV69Wj179tRrr72mMWPGaPfu3Wrfvr0tMYiMjNSUKVMkSUOGDNHSpUu1dOlStWvXztbPmTNn1K1bN7Vo0UKvv/66OnbsWGR8b7zxhqpVq6a4uDjl5eVJkt5++21t2LBBc+bMUWhoaLHvFXB7BoBiO3funCHJ6N27d7Ha79q1y5BkDBo0yG7/6NGjDUnG5s2bbfvCw8MNSUZCQoJt36lTpwyr1WqMGjXKtu/w4cOGJGPGjBl2fcbFxRnh4eGFYpg4caLx57/qs2bNMiQZp0+fvmrcBddYuHChbV+LFi2M6tWrG2fOnLHt+/HHHw0PDw/jscceK3S9J554wq7Pe++916hSpcpVr/nn+/D19TUMwzDuu+8+o1OnToZhGEZeXp4RHBxsTJ48ucjvICsry8jLyyt0H1ar1ZgyZYpt3/bt2wvdW4H27dsbkoz58+cXeax9+/Z2+9avX29IMl5++WXj0KFDRsWKFY0+ffpc9x4B2KMyAjggPT1dkuTn51es9p9++qkkaeTIkXb7R40aJUmF5pY0btxYd999t+1ztWrV1LBhQx06dOiGY75SwVyTjz/+WPn5+cU658SJE9q1a5cGDBigypUr2/Y3a9ZMnTt3tt3nnz355JN2n++++26dOXPG9h0Wx8MPP6wvv/xSKSkp2rx5s1JSUoocopEuzzPx8Lj8T1peXp7OnDljG4L64Ycfin1Nq9Wqxx9/vFhtu3Tpor/97W+aMmWK+vbtK29vb7399tvFvhaAy0hGAAf4+/tLks6fP1+s9kePHpWHh4fq1atntz84OFiBgYE6evSo3f5atWoV6qNSpUo6e/bsDUZc2IMPPqi2bdtq0KBBCgoKUv/+/fX+++9fMzEpiLNhw4aFjkVGRuqPP/5QZmam3f4r76VSpUqS5NC9dO/eXX5+flq5cqWWLVumO+64o9B3WSA/P1+zZs1S/fr1ZbVaVbVqVVWrVk0//fSTzp07V+xr1qhRw6HJqjNnzlTlypW1a9cuzZ49W9WrVy/2uQAuIxkBHODv76/Q0FD9/PPPDp135QTSq/H09Cxyv2EYN3yNgvkMBXx8fJSQkKDPP/9cjz76qH766Sc9+OCD6ty5c6G2JVGSeylgtVrVt29fLV68WKtWrbpqVUSSXnnlFY0cOVLt2rXTv/71L61fv14bN27UbbfdVuwKkHT5+3HEzp07derUKUnS7t27HToXwGUkI4CDevbsqaSkJCUmJl63bXh4uPLz83XgwAG7/SdPnlRaWpptZYwzVKpUyW7lSYErqy+S5OHhoU6dOum1117T3r17NXXqVG3evFlffPFFkX0XxLl///5Cx3755RdVrVpVvr6+JbuBq3j44Ye1c+dOnT9/vshJvwX+/e9/q2PHjnr33XfVv39/denSRdHR0YW+k+ImhsWRmZmpxx9/XI0bN9aQIUM0ffp0bd++3Wn9A+6CZARw0NixY+Xr66tBgwbp5MmThY4nJSXpjTfekHR5mEFSoRUvr732miSpR48eTourbt26OnfunH766SfbvhMnTmjVqlV27VJTUwudW/DwryuXGxcICQlRixYttHjxYrtf7j///LM2bNhgu8/S0LFjR7300kt68803FRwcfNV2np6ehaouH3zwgX7//Xe7fQVJU1GJm6PGjRunY8eOafHixXrttddUu3ZtxcXFXfV7BFA0HnoGOKhu3bpavny5HnzwQUVGRto9gXXr1q364IMPNGDAAElS8+bNFRcXp3/+859KS0tT+/bt9d1332nx4sXq06fPVZeN3oj+/ftr3LhxuvfeezVs2DBduHBB8+bNU4MGDewmcE6ZMkUJCQnq0aOHwsPDderUKb311luqWbOm7rrrrqv2P2PGDHXr1k1RUVEaOHCgLl68qDlz5iggIECTJk1y2n1cycPDQy+++OJ12/Xs2VNTpkzR448/rjvvvFO7d+/WsmXLVKdOHbt2devWVWBgoObPny8/Pz/5+vqqTZs2ioiIcCiuzZs366233tLEiRNtS40XLlyoDh06aPz48Zo+fbpD/QFuzeTVPECZ9euvvxqDBw82ateubXh5eRl+fn5G27ZtjTlz5hhZWVm2drm5ucbkyZONiIgIo3z58kZYWJjx/PPP27UxjMtLe3v06FHoOlcuKb3a0l7DMIwNGzYYTZo0Mby8vIyGDRsa//rXvwot7d20aZPRu3dvIzQ01PDy8jJCQ0ONhx56yPj1118LXePK5a+ff/650bZtW8PHx8fw9/c3evXqZezdu9euTcH1rlw6vHDhQkOScfjw4at+p4Zhv7T3aq62tHfUqFFGSEiI4ePjY7Rt29ZITEwscknuxx9/bDRu3NgoV66c3X22b9/euO2224q85p/7SU9PN8LDw43bb7/dyM3NtWs3YsQIw8PDw0hMTLzmPQD4L4thODCbDAAAwMmYMwIAAExFMgIAAExFMgIAAExFMgIAAExFMgIAgJuaNm2a7rjjDvn5+al69erq06dPoYcbdujQQRaLxW678t1Tx44dU48ePVShQgVVr15dY8aM0aVLl4odB88ZAQDATW3ZskXx8fG64447dOnSJf39739Xly5dtHfvXrunKg8ePFhTpkyxfa5QoYLt57y8PPXo0UPBwcHaunWrTpw4occee0zly5fXK6+8Uqw4WNpbyvLz85WcnCw/Pz+nPoYaAHBzGIah8+fPKzQ01PZmaGfLyspSTk6OU/ry8vKSt7f3DZ17+vRpVa9eXVu2bFG7du0kXa6MtGjRotCTpAt89tln6tmzp5KTkxUUFCRJmj9/vsaNG6fTp08X78WTpj7lxA0cP37ckMTGxsbGVsa348ePl8rviYsXLxoqV8FpcQYHBxsXL168oVgOHDhgSDJ2795t29e+fXujatWqRpUqVYzbbrvNeO6554zMzEzb8fHjxxvNmze36+fQoUOGJOOHH34o1nUZpillfn5+kiSvxnGyeBb/teRAWXLsy5lmhwCUmvPp6aoXEWb799zZcnJypEsXZG0cJ5X090RejlL2LtYff/whf39/226r1Sqr1XrNU/Pz8zV8+HC1bdtWTZo0se1/+OGHFR4ertDQUP30008aN26c9u/fr48++kiSlJKSYquIFCj4nJKSUqywSUZKWcHQjMXTi2QEt6w//6MH3KpKfai9nHeJf08YlsvDSGFhYXb7J06ceN13SMXHx+vnn3/W119/bbd/yJAhtp+bNm2qkJAQderUSUlJSapbt26J4i1AMgIAgCuwSCppwvP/Tz9+/Hihysi1DB06VGvXrlVCQoJq1qx5zbZt2rSRJB08eFB169ZVcHCwvvvuO7s2BW80v9abtv+Mpb0AALgCi4dzNl2uVv55u1oyYhiGhg4dqlWrVmnz5s3Fenv1rl27JEkhISGSpKioKO3evVunTp2ytdm4caP8/f3VuHHjYt06lREAANxUfHy8li9fro8//lh+fn62OR4BAQHy8fFRUlKSli9fru7du6tKlSr66aefNGLECLVr107NmjWTJHXp0kWNGzfWo48+qunTpyslJUUvvvii4uPjr1uRKUBlBAAAV2CxOGdzwLx583Tu3Dl16NBBISEhtm3lypWSLi8T/vzzz9WlSxc1atRIo0aNUr9+/bRmzRpbH56enlq7dq08PT0VFRWlRx55RI899pjdc0muh8oIAACu4E/DLCXqwwHGdR41FhYWpi1btly3n/DwcH366acOXfvPqIwAAABTURkBAMAV3MAwS5F9lEEkIwAAuAQnDNOU0QGPshk1AAC4ZVAZAQDAFTBMAwAATGXCahpXUTajBgAAtwwqIwAAuAKGaQAAgKnceJiGZAQAAFfgxpWRsplCAQCAWwaVEQAAXAHDNAAAwFQWixOSEYZpAAAAHEZlBAAAV+BhubyVtI8yiGQEAABX4MZzRspm1AAA4JZBZQQAAFfgxs8ZIRkBAMAVMEwDAABgDiojAAC4AoZpAACAqdx4mIZkBAAAV+DGlZGymUIBAIBbBpURAABcAcM0AADAVAzTAAAAmIPKCAAALsEJwzRltMZAMgIAgCtgmAYAAMAcVEYAAHAFFosTVtOUzcoIyQgAAK7AjZf2ls2oAQDALYPKCAAArsCNJ7CSjAAA4ArceJiGZAQAAFfgxpWRsplCAQCAWwaVEQAAXAHDNAAAwFQM0wAAAJiDyggAAC7AYrHI4qaVEZIRAABcgDsnIwzTAAAAU1EZAQDAFVj+/1bSPsogkhEAAFwAwzQAAAAmoTICAIALcOfKCMkIAAAugGQEAACYyp2TEeaMAAAAU1EZAQDAFbC0FwAAmIlhGgAAAJNQGQEAwAVYLHJCZcQ5sdxsJCMAALgAi5wwTFNGsxGGaQAAgKmojAAA4ALceQIryQgAAK7AjZf2MkwDAABMRWUEAABX4IRhGoNhGgAAcKOcMWek5KtxzEEyAgCAC3DnZIQ5IwAAwFQkIwAAuAKLkzYHTJs2TXfccYf8/PxUvXp19enTR/v377drk5WVpfj4eFWpUkUVK1ZUv379dPLkSbs2x44dU48ePVShQgVVr15dY8aM0aVLl4odB8kIAAAuoGCYpqSbI7Zs2aL4+Hh9++232rhxo3Jzc9WlSxdlZmba2owYMUJr1qzRBx98oC1btig5OVl9+/a1Hc/Ly1OPHj2Uk5OjrVu3avHixVq0aJEmTJhQ/Hs3DMNwKHI4JD09XQEBAbI2HSyLp5fZ4QCl4uz2N80OASg16enpCqoSoHPnzsnf379U+g8ICFDVRxfJw6tCifrKz7mgP5YOuOFYT58+rerVq2vLli1q166dzp07p2rVqmn58uW67777JEm//PKLIiMjlZiYqL/+9a/67LPP1LNnTyUnJysoKEiSNH/+fI0bN06nT5+Wl9f1f/dRGQEAwAWYURm50rlz5yRJlStXliR9//33ys3NVXR0tK1No0aNVKtWLSUmJkqSEhMT1bRpU1siIkkxMTFKT0/Xnj17inVdVtMAAOACnLmaJj093W6/1WqV1Wq95rn5+fkaPny42rZtqyZNmkiSUlJS5OXlpcDAQLu2QUFBSklJsbX5cyJScLzgWHFQGQEA4BYTFhamgIAA2zZt2rTrnhMfH6+ff/5ZK1asuAkR2qMyAgCAC3BmZeT48eN2c0auVxUZOnSo1q5dq4SEBNWsWdO2Pzg4WDk5OUpLS7Orjpw8eVLBwcG2Nt99951dfwWrbQraXA+VEQAAXIETl/b6+/vbbVdLRgzD0NChQ7Vq1Spt3rxZERERdsdbtWql8uXLa9OmTbZ9+/fv17FjxxQVFSVJioqK0u7du3Xq1Clbm40bN8rf31+NGzcu1q1TGQEAwE3Fx8dr+fLl+vjjj+Xn52eb4xEQECAfHx8FBARo4MCBGjlypCpXrix/f38988wzioqK0l//+ldJUpcuXdS4cWM9+uijmj59ulJSUvTiiy8qPj7+uhWZAiQjAAC4ADMeBz9v3jxJUocOHez2L1y4UAMGDJAkzZo1Sx4eHurXr5+ys7MVExOjt956y9bW09NTa9eu1VNPPaWoqCj5+voqLi5OU6ZMKXYcJCMAALgAM5KR4jxqzNvbW3PnztXcuXOv2iY8PFyffvqpQ9f+M5IRAABcAC/KAwAAMAmVEQAAXMENvOiuyD7KIJIRAABcAMM0AAAAJnGLysiiRYs0fPhwpaWlmR0KbsCIAV3Us2Nz1Q8PUlZ2rr776ZAmvfmxDh797wN21sx/Vne1qm933sIPv9bIf/z3scb/GHWf2jSvo8i6Ifr1yEm1i/3HTbsHoKS++eGg5iz9XD/+ckwpf6TrXzMGq0eH5maHBSeiMlJGHD9+XE888YRCQ0Pl5eWl8PBwPfvsszpz5oytTe3atfX666+bFySc7s7b6+mdDxLU5YmZ6jv0TZUv56mP5gxVBW/711IvWvWNGnZ93rZNnLO6UF/L1nyrVRt/uEmRA85z4WK2mjSooRljHzQ7FJQSi5zw1t4yOmmkzFRGDh06pKioKDVo0EDvvfeeIiIitGfPHo0ZM0afffaZvv32W9srj2+W3NxclS9f/qZe0x3dP+wtu89PT/6XDm78h1pEhmnrziTb/otZOTp15vxV+3nu1X9LkqoEdtdt9WuUTrBAKenc9jZ1bnub2WEApaLMVEbi4+Pl5eWlDRs2qH379qpVq5a6deumzz//XL///rteeOEFdejQQUePHtWIESOKLHetX79ekZGRqlixorp27aoTJ07YHX/nnXcUGRkpb29vNWrUyO4Jc0eOHJHFYtHKlSvVvn17eXt7a9myZTfl3mHPv6K3JOls+gW7/fd3ba2DG/+hrSv+rgnx98jHSqIIoOwocVXECcM8ZikTlZHU1FStX79eU6dOlY+Pj92x4OBgxcbGauXKlTpw4IBatGihIUOGaPDgwXbtLly4oJkzZ2rp0qXy8PDQI488otGjR9sSimXLlmnChAl688031bJlS+3cuVODBw+2Pda2wHPPPadXX31VLVu2lLe3d+nfPOxYLBZNG3mfvt2VpH1J/00m/71+h46fSFXK6XO6rX6oJg7trXrh1fXY2HdMjBYAHMDSXtd24MABGYahyMjIIo9HRkbq7NmzysvLk6enp/z8/Aq9tjg3N1fz589X3bp1JV1+XfKfn5s/ceJEvfrqq+rbt68kKSIiQnv37tXbb79tl4wMHz7c1qYo2dnZys7Otn1OT093/IZxVTPHPqDIuiHqNniW3f7Fq76x/bw3KVkpf6TrP/OGqXaNqjry+x83O0wAgAPKRDJSoDjP0L+aChUq2BIRSQoJCbG97jgzM1NJSUkaOHCgXUXl0qVLCggIsOundevW17zOtGnTNHny5BuOE1c3fcz9irm7iboPeV3Jp9Ku2fb7n49IkuqEVSMZAVAmuPNqmjKRjNSrV08Wi0X79u3TvffeW+j4vn37VKlSJVWrVu2qfVw50dRisdiSm4yMDEnSggUL1KZNG7t2np6edp99fX2vGevzzz+vkSNH2j6np6crLCzsmufg+qaPuV89OjRXryff0LHkM9dt37RBTUnSyT/OlXZoAOAUJCMurkqVKurcubPeeustjRgxwm7eSEpKipYtW6bHHntMFotFXl5eysvLc6j/oKAghYaG6tChQ4qNjS1RrFarVVartUR9wN7McQ/ovpjWenj0P5VxIUvVq/hJktIzspSVnavaNarqvq6ttfGbPUo9l6km9Wto6oi++uaHA9pzMNnWT0TNqvKtYFVQFX95W8urSYPLK2r2H0pR7iXH/swAN1vGhWwdPn7a9vlo8hnt3v+bAgMqKCz45q4kROmwWC5vJe2jLCoTyYgkvfnmm7rzzjsVExOjl19+2W5pb40aNTR16lRJl58zkpCQoP79+8tqtapq1arF6n/y5MkaNmyYAgIC1LVrV2VnZ2vHjh06e/asXaUDN9/A+9pJkj55e7jd/qcnL9V7a7cp99IldfhLQz3Vv6Mq+Hjp95NntWbzLs38v/V27We/GGv3YLSvlj0vSWp2zwQdP5FaujcBlNCufUfV68nZts8vzPpIkvRQjzZ6a9KjZoUFOEWZSUbq16+vHTt2aOLEiXrggQeUmpqq4OBg9enTRxMnTrQ9Y2TKlCn629/+prp16yo7O7vY80wGDRqkChUqaMaMGRozZox8fX3VtGlTDR8+vBTvCsVR6Y6h1zz++8k09fzbG9ftp9eT128DuKq7WjXQ2e1vmh0GStHlykhJh2mcFMxNZjFKMisU15Wenq6AgABZmw6WxdPr+icAZRC/JHErS09PV1CVAJ07d07+/v6l0n9AQIDqDPu3PK3Xnpd4PXnZmTo0+75Si7W0lJmHngEAgFtTmRmmAQDgVsZqGgAAYCp3Xk3DMA0AADAVlREAAFyAh4dFHh4lK20YJTzfLCQjAAC4AIZpAAAATEJlBAAAF8BqGgAAYCp3HqYhGQEAwAW4c2WEOSMAAMBUVEYAAHAB7lwZIRkBAMAFuPOcEYZpAACAqaiMAADgAixywjCNymZphGQEAAAXwDANAACASaiMAADgAlhNAwAATMUwDQAAgEmojAAA4AIYpgEAAKZy52EakhEAAFyAO1dGmDMCAABMRWUEAABX4IRhmjL6AFaSEQAAXAHDNAAAACahMgIAgAtgNQ0AADAVwzQAAAAmoTICAIALYJgGAACYimEaAAAAk1AZAQDABbhzZYRkBAAAF8CcEQAAYCp3rowwZwQAAJiKyggAAC6AYRoAAGAqhmkAAABMQmUEAAAXYJEThmmcEsnNRzICAIAL8LBY5FHCbKSk55uFYRoAAGAqKiMAALgAVtMAAABTufNqGpIRAABcgIfl8lbSPsoi5owAAOCmEhIS1KtXL4WGhspisWj16tV2xwcMGGCr2BRsXbt2tWuTmpqq2NhY+fv7KzAwUAMHDlRGRoZDcZCMAADgCiwq9Ivf0c3Rtb2ZmZlq3ry55s6de9U2Xbt21YkTJ2zbe++9Z3c8NjZWe/bs0caNG7V27VolJCRoyJAhDsXBMA0AAC7AjAms3bp1U7du3a7Zxmq1Kjg4uMhj+/bt07p167R9+3a1bt1akjRnzhx1795dM2fOVGhoaLHioDICAMAtJj093W7Lzs6+4b6+/PJLVa9eXQ0bNtRTTz2lM2fO2I4lJiYqMDDQlohIUnR0tDw8PLRt27ZiX4NkBAAAF2Bx0n+SFBYWpoCAANs2bdq0G4qpa9euWrJkiTZt2qT//d//1ZYtW9StWzfl5eVJklJSUlS9enW7c8qVK6fKlSsrJSWl2NdhmAYAABfgzNU0x48fl7+/v22/1Wq9of769+9v+7lp06Zq1qyZ6tatqy+//FKdOnUqUax/RmUEAIBbjL+/v912o8nIlerUqaOqVavq4MGDkqTg4GCdOnXKrs2lS5eUmpp61XkmRSEZAQDABZR0JY0zHpp2Pb/99pvOnDmjkJAQSVJUVJTS0tL0/fff29ps3rxZ+fn5atOmTbH7LdYwzX/+859id3jPPfcUuy0AALjMjNU0GRkZtiqHJB0+fFi7du1S5cqVVblyZU2ePFn9+vVTcHCwkpKSNHbsWNWrV08xMTGSpMjISHXt2lWDBw/W/PnzlZubq6FDh6p///7FXkkjFTMZ6dOnT7E6s1gstkktAADAte3YsUMdO3a0fR45cqQkKS4uTvPmzdNPP/2kxYsXKy0tTaGhoerSpYteeuklu2GfZcuWaejQoerUqZM8PDzUr18/zZ4926E4ipWM5OfnO9QpAABwjIfFIo8SlkYcPb9Dhw4yDOOqx9evX3/dPipXrqzly5c7dN0rlWg1TVZWlry9vUsUAAAAcO+39jo8gTUvL08vvfSSatSooYoVK+rQoUOSpPHjx+vdd991eoAAALiDsjCBtbQ4nIxMnTpVixYt0vTp0+Xl5WXb36RJE73zzjtODQ4AANz6HE5GlixZon/+85+KjY2Vp6enbX/z5s31yy+/ODU4AADcRcEwTUm3ssjhOSO///676tWrV2h/fn6+cnNznRIUAADuxowJrK7C4cpI48aN9dVXXxXa/+9//1stW7Z0SlAAAMB9OFwZmTBhguLi4vT7778rPz9fH330kfbv368lS5Zo7dq1pREjAAC3PMv/30raR1nkcGWkd+/eWrNmjT7//HP5+vpqwoQJ2rdvn9asWaPOnTuXRowAANzy3Hk1zQ09Z+Tuu+/Wxo0bnR0LAABwQzf80LMdO3Zo3759ki7PI2nVqpXTggIAwN14WC5vJe2jLHI4Gfntt9/00EMP6ZtvvlFgYKAkKS0tTXfeeadWrFihmjVrOjtGAABuec4YZimrwzQOzxkZNGiQcnNztW/fPqWmpio1NVX79u1Tfn6+Bg0aVBoxAgCAW5jDlZEtW7Zo69atatiwoW1fw4YNNWfOHN19991ODQ4AAHdSRgsbJeZwMhIWFlbkw83y8vIUGhrqlKAAAHA3DNM4YMaMGXrmmWe0Y8cO274dO3bo2Wef1cyZM50aHAAA7qJgAmtJt7KoWJWRSpUq2WVbmZmZatOmjcqVu3z6pUuXVK5cOT3xxBPq06dPqQQKAABuTcVKRl5//fVSDgMAAPfmzsM0xUpG4uLiSjsOAADcmjs/Dv6GH3omSVlZWcrJybHb5+/vX6KAAACAe3E4GcnMzNS4ceP0/vvv68yZM4WO5+XlOSUwAADciYfFIo8SDrOU9HyzOLyaZuzYsdq8ebPmzZsnq9Wqd955R5MnT1ZoaKiWLFlSGjECAHDLs1ics5VFDldG1qxZoyVLlqhDhw56/PHHdffdd6tevXoKDw/XsmXLFBsbWxpxAgCAW5TDlZHU1FTVqVNH0uX5IampqZKku+66SwkJCc6NDgAAN1GwmqakW1nkcDJSp04dHT58WJLUqFEjvf/++5IuV0wKXpwHAAAc487DNA4nI48//rh+/PFHSdJzzz2nuXPnytvbWyNGjNCYMWOcHiAAALi1OTxnZMSIEbafo6Oj9csvv+j7779XvXr11KxZM6cGBwCAu3Dn1TQles6IJIWHhys8PNwZsQAA4LacMcxSRnOR4iUjs2fPLnaHw4YNu+FgAABwVzwO/jpmzZpVrM4sFgvJCAAAcEixkpGC1TO4cQc2/i+Pysct6x+bDpgdAlBqsjMzbsp1PHQDq0qK6KMsKvGcEQAAUHLuPExTVpMoAABwi6AyAgCAC7BYJA9W0wAAALN4OCEZKen5ZmGYBgAAmOqGkpGvvvpKjzzyiKKiovT7779LkpYuXaqvv/7aqcEBAOAueFGeAz788EPFxMTIx8dHO3fuVHZ2tiTp3LlzeuWVV5weIAAA7qBgmKakW1nkcDLy8ssva/78+VqwYIHKly9v29+2bVv98MMPTg0OAADc+hyewLp//361a9eu0P6AgAClpaU5IyYAANyOO7+bxuHKSHBwsA4ePFho/9dff606deo4JSgAANxNwVt7S7qVRQ4nI4MHD9azzz6rbdu2yWKxKDk5WcuWLdPo0aP11FNPlUaMAADc8jyctJVFDg/TPPfcc8rPz1enTp104cIFtWvXTlarVaNHj9YzzzxTGjECAIBbmMPJiMVi0QsvvKAxY8bo4MGDysjIUOPGjVWxYsXSiA8AALfgznNGbvgJrF5eXmrcuLEzYwEAwG15qORzPjxUNrMRh5ORjh07XvOhKps3by5RQAAAwL04nIy0aNHC7nNubq527dqln3/+WXFxcc6KCwAAt8IwjQNmzZpV5P5JkyYpIyOjxAEBAOCOeFGeEzzyyCP6v//7P2d1BwAA3MQNT2C9UmJiory9vZ3VHQAAbsViUYknsLrNME3fvn3tPhuGoRMnTmjHjh0aP3680wIDAMCdMGfEAQEBAXafPTw81LBhQ02ZMkVdunRxWmAAAMA9OJSM5OXl6fHHH1fTpk1VqVKl0ooJAAC3wwTWYvL09FSXLl14Oy8AAE5mcdJ/ZZHDq2maNGmiQ4cOlUYsAAC4rYLKSEm3ssjhZOTll1/W6NGjtXbtWp04cULp6el2GwAAgCOKPWdkypQpGjVqlLp37y5Juueee+weC28YhiwWi/Ly8pwfJQAAtzh3njNS7GRk8uTJevLJJ/XFF1+UZjwAALgli8VyzXe/FbePsqjYyYhhGJKk9u3bl1owAADA/Ti0tLesZlwAALg6hmmKqUGDBtdNSFJTU0sUEAAA7ognsBbT5MmTCz2BFQAAoCQcSkb69++v6tWrl1YsAAC4LQ+LpcQvyivp+WYpdjLCfBEAAEqPO88ZKfZDzwpW0wAAgFtDQkKCevXqpdDQUFksFq1evdruuGEYmjBhgkJCQuTj46Po6GgdOHDArk1qaqpiY2Pl7++vwMBADRw4UBkZGQ7FUexkJD8/nyEaAABKi+W/k1hvdHP01TSZmZlq3ry55s6dW+Tx6dOna/bs2Zo/f762bdsmX19fxcTEKCsry9YmNjZWe/bs0caNG7V27VolJCRoyJAhDsXh0JwRAABQOjxkkUcJX3Tn6PndunVTt27dijxmGIZef/11vfjii+rdu7ckacmSJQoKCtLq1avVv39/7du3T+vWrdP27dvVunVrSdKcOXPUvXt3zZw5U6GhocWMGwAAmK6kVZE/Lw2+8r1x2dnZDsdz+PBhpaSkKDo62rYvICBAbdq0UWJioiQpMTFRgYGBtkREkqKjo+Xh4aFt27YV+1okIwAA3GLCwsIUEBBg26ZNm+ZwHykpKZKkoKAgu/1BQUG2YykpKYWmcJQrV06VK1e2tSkOhmkAAHABzlxNc/z4cfn7+9v2W63WknVcykhGAABwAc58zoi/v79dMnIjgoODJUknT55USEiIbf/JkyfVokULW5tTp07ZnXfp0iWlpqbazi9W3CWKFAAA3JIiIiIUHBysTZs22falp6dr27ZtioqKkiRFRUUpLS1N33//va3N5s2blZ+frzZt2hT7WlRGAABwAWa8myYjI0MHDx60fT58+LB27dqlypUrq1atWho+fLhefvll1a9fXxERERo/frxCQ0PVp08fSVJkZKS6du2qwYMHa/78+crNzdXQoUPVv3//Yq+kkUhGAABwCR5ywjCNg0t7d+zYoY4dO9o+jxw5UpIUFxenRYsWaezYscrMzNSQIUOUlpamu+66S+vWrZO3t7ftnGXLlmno0KHq1KmTPDw81K9fP82ePduhOEhGAABwUx06dLjmE9YtFoumTJmiKVOmXLVN5cqVtXz58hLFQTICAIALMGOYxlWQjAAA4AI8VPJVJWV1VUpZjRsAANwiqIwAAOACLBaLLCUcZynp+WYhGQEAwAXcwEt3i+yjLCIZAQDABTjzCaxlDXNGAACAqaiMAADgIspmXaPkSEYAAHAB7vycEYZpAACAqaiMAADgAljaCwAATMUTWAEAAExCZQQAABfAMA0AADCVOz+BlWEaAABgKiojAAC4AIZpAACAqdx5NQ3JCAAALsCdKyNlNYkCAAC3CCojAAC4AHdeTUMyAgCAC+BFeQAAACahMgIAgAvwkEUeJRxoKen5ZiEZAQDABTBMAwAAYBIqIwAAuADL//+vpH2URSQjAAC4AIZpAAAATEJlBAAAF2BxwmoahmkAAMANc+dhGpIRAABcgDsnI8wZAQAApqIyAgCAC2BpLwAAMJWH5fJW0j7KIoZpAACAqaiMAADgAhimAQAApmI1DQAAgEmojAAA4AIsKvkwSxktjJCMAADgClhNAwAAYJIyURmxWCxatWqV+vTpY3YocGEZmVn63wWf6tMtP+nM2Qw1aVBDLw3vq5aNw80ODbim44d+17aE73Xy91PKOJ+pex/tqQa31bUd3//zQe3atlspv59S1oUsDRj2sIJCqxXZl2EY+mDhxzr869FC/cC1ufNqGtMrIykpKXrmmWdUp04dWa1WhYWFqVevXtq0aZPZoUm6/Bd7woQJCgkJkY+Pj6Kjo3XgwAGzw0IRRv5jhbZs3683JzyiL/41Tu3/0kgPPPuWTpxOMzs04JpycnNVPaSqOvfuUOTx3Jxc1QwPVYeuba/b146vd5bZFRXurmA1TUm3ssjUZOTIkSNq1aqVNm/erBkzZmj37t1at26dOnbsqPj4eDNDs5k+fbpmz56t+fPna9u2bfL19VVMTIyysrLMDg1/cjE7R598+aPGP32PolrWU0TNahozqJsialbV4o++MTs84JrqNqytdjF3qkGTekUeb3J7pNpGt1HterWu2c/J5NP67qud6nZf59IIE6XM4qStLDI1GXn66adlsVj03XffqV+/fmrQoIFuu+02jRw5Ut9+++1Vzxs3bpwaNGigChUqqE6dOho/frxyc3Ntx3/88Ud17NhRfn5+8vf3V6tWrbRjxw5J0tGjR9WrVy9VqlRJvr6+uu222/Tpp58WeR3DMPT666/rxRdfVO/evdWsWTMtWbJEycnJWr16tVO/C5RM3qV85eXly9tqP/LobS2vbT8dMikq4ObJzcnVmhXr1KV3B1X08zU7HMAhps0ZSU1N1bp16zR16lT5+hb+ixMYGHjVc/38/LRo0SKFhoZq9+7dGjx4sPz8/DR27FhJUmxsrFq2bKl58+bJ09NTu3btUvny5SVJ8fHxysnJUUJCgnx9fbV3715VrFixyOscPnxYKSkpio6Otu0LCAhQmzZtlJiYqP79+xc6Jzs7W9nZ2bbP6enpxfo+UDIVfb3VukltvbZwg+qHB6taZT+t2vi9dvx8RBE1ix5bB24lm9YmqEatENVnjkiZ5SGLPEo4zuJRRmsjpiUjBw8elGEYatSokcPnvvjii7afa9eurdGjR2vFihW2ZOTYsWMaM2aMre/69evb2h87dkz9+vVT06ZNJUl16tS56nVSUlIkSUFBQXb7g4KCbMeuNG3aNE2ePNnhe0LJvTnhUQ1/Zbla9J4gT08PNW1QU/dG366f9v9mdmhAqTqw95COJR3XgGEPmx0KSsAZwyxlMxUxMRkxDOOGz125cqVmz56tpKQkZWRk6NKlS/L397cdHzlypAYNGqSlS5cqOjpa999/v+rWvfx/C8OGDdNTTz2lDRs2KDo6Wv369VOzZs1KfD8Fnn/+eY0cOdL2OT09XWFhYU7rH1dXu2ZVrX5rmDIvZisjM0tBVQM0ZPwi1QqtYnZoQKk6mnRcZ1PP6fXJ8+32r/7XJ6pZO1QP/+0+kyIDise0OSP169eXxWLRL7/84tB5iYmJio2NVffu3bV27Vrt3LlTL7zwgnJycmxtJk2apD179qhHjx7avHmzGjdurFWrVkmSBg0apEOHDunRRx/V7t271bp1a82ZM6fIawUHB0uSTp48abf/5MmTtmNXslqt8vf3t9twc/n6WBVUNUBp6Rf05bZf1PXupmaHBJSqv3ZorSeejdXjwx62bZL0Pz3bqfv9TGYtM9x4BqtpyUjlypUVExOjuXPnKjMzs9DxtLS0Is/bunWrwsPD9cILL6h169aqX7++jh49WqhdgwYNNGLECG3YsEF9+/bVwoULbcfCwsL05JNP6qOPPtKoUaO0YMGCIq8VERGh4OBgu2XG6enp2rZtm6Kiohy8Y5S2L77dp83f7tPR5DPa8t0v6vfMm6oXXl39e7YxOzTgmnKyc3Qy+bROJp+WJJ1LPaeTyaeVnnZ5ztnFC1k6mXxaf5w6I0lKPX1WJ5NPK+P85X87K/r5qlpwVbtNkvwD/RRYOcCEO8KNsDjpv7LI1IeezZ07V23bttVf/vIXTZkyRc2aNdOlS5e0ceNGzZs3T/v27St0Tv369XXs2DGtWLFCd9xxhz755BNb1UOSLl68qDFjxui+++5TRESEfvvtN23fvl39+vWTJA0fPlzdunVTgwYNdPbsWX3xxReKjIwsMj6LxaLhw4fr5ZdfVv369RUREaHx48crNDSUB7C5oPTMLL0yb41OnE5ToL+venRoruf/1kPly3maHRpwTSm/ndJ7Cz60fd78yVeSLi/p7fFAFx3ce0if/nuj7fh/3vtMktS2Uxvd1fmvNzdYoBSYmozUqVNHP/zwg6ZOnapRo0bpxIkTqlatmlq1aqV58+YVec4999yjESNGaOjQocrOzlaPHj00fvx4TZo0SZLk6empM2fO6LHHHtPJkydVtWpV9e3b1zapNC8vT/Hx8frtt9/k7++vrl27atasWVeNcezYscrMzNSQIUOUlpamu+66S+vWrZO3t7fTvw+UTO9OLdW7U0uzwwAcVqtuTY37x7NXPd60dWM1bd3YoT6v1R9clDMeWlY2CyOyGCWZSYrrSk9PV0BAgI6lpDJ/BLesWQk8ywW3ruzMDP2j3+06d+5cqfw7XvB7YvOuY6roV7L+M86n639a1Cq1WEuL6Y+DBwAA7q1MvCgPAIBbnhs/aIRkBAAAF+DOb+0lGQEAwAU44627vLUXAADgBlAZAQDABbjxlBGSEQAAXIIbZyMM0wAAAFNRGQEAwAW482oaKiMAALiAgtU0Jd0cMWnSJFksFrutUaNGtuNZWVmKj49XlSpVVLFiRfXr16/Qm+ydgWQEAAA3dtttt+nEiRO27euvv7YdGzFihNasWaMPPvhAW7ZsUXJysvr27ev0GBimAQDABZg1f7VcuXIKDg4utP/cuXN69913tXz5cv3P//yPJGnhwoWKjIzUt99+q7/+1XlvjKYyAgCAK7A4adPll+/9ecvOzr7qZQ8cOKDQ0FDVqVNHsbGxOnbsmCTp+++/V25urqKjo21tGzVqpFq1aikxMdGZd04yAgDArSYsLEwBAQG2bdq0aUW2a9OmjRYtWqR169Zp3rx5Onz4sO6++26dP39eKSkp8vLyUmBgoN05QUFBSklJcWq8DNMAAOACnLma5vjx4/L397ftt1qtRbbv1q2b7edmzZqpTZs2Cg8P1/vvvy8fH58SxeIIKiMAALgAZ66m8ff3t9uuloxcKTAwUA0aNNDBgwcVHBysnJwcpaWl2bU5efJkkXNMSoJkBAAAF+DEKSM3LCMjQ0lJSQoJCVGrVq1Uvnx5bdq0yXZ8//79OnbsmKKiokp4JXsM0wAA4KZGjx6tXr16KTw8XMnJyZo4caI8PT310EMPKSAgQAMHDtTIkSNVuXJl+fv765lnnlFUVJRTV9JIJCMAALgGE9b2/vbbb3rooYd05swZVatWTXfddZe+/fZbVatWTZI0a9YseXh4qF+/fsrOzlZMTIzeeuutEgZZGMkIAAAuwIzHwa9YseKax729vTV37lzNnTu3JGFdF3NGAACAqaiMAADgAm7k3TJF9VEWkYwAAOACzHocvCtgmAYAAJiKyggAAK7AjUsjJCMAALgAM1bTuAqGaQAAgKmojAAA4AJYTQMAAEzlxlNGSEYAAHAJbpyNMGcEAACYisoIAAAuwJ1X05CMAADgCpwwgbWM5iIM0wAAAHNRGQEAwAW48fxVkhEAAFyCG2cjDNMAAABTURkBAMAFsJoGAACYyp0fB88wDQAAMBWVEQAAXIAbz18lGQEAwCW4cTZCMgIAgAtw5wmszBkBAACmojICAIALsMgJq2mcEsnNRzICAIALcOMpIwzTAAAAc1EZAQDABbjzQ89IRgAAcAnuO1DDMA0AADAVlREAAFwAwzQAAMBU7jtIwzANAAAwGZURAABcAMM0AADAVO78bhqSEQAAXIEbTxphzggAADAVlREAAFyAGxdGSEYAAHAF7jyBlWEaAABgKiojAAC4AFbTAAAAc7nxpBGGaQAAgKmojAAA4ALcuDBCMgIAgCtgNQ0AAIBJqIwAAOASSr6apqwO1JCMAADgAhimAQAAMAnJCAAAMBXDNAAAuAB3HqYhGQEAwAW48+PgGaYBAACmojICAIALYJgGAACYyp0fB88wDQAAMBWVEQAAXIEbl0ZIRgAAcAGspgEAADAJlREAAFwAq2kAAICp3HjKCMkIAAAuwY2zEeaMAAAAU1EZAQDABbjzahqSEQAAXAATWFFqDMOQJJ0/n25yJEDpyc7MMDsEoNRkX7j857vg3/PSkp5e8t8TzujDDCQjpez8+fOSpNvq1zY3EABAiZw/f14BAQFO79fLy0vBwcGqHxHmlP6Cg4Pl5eXllL5uFotR2qmem8vPz1dycrL8/PxkKav1szIkPT1dYWFhOn78uPz9/c0OB3A6/ozffIZh6Pz58woNDZWHR+ms+8jKylJOTo5T+vLy8pK3t7dT+rpZqIyUMg8PD9WsWdPsMNyOv78//1Djlsaf8ZurNCoif+bt7V3mEghnYmkvAAAwFckIAAAwFckIbilWq1UTJ06U1Wo1OxSgVPBnHLciJrACAABTURkBAACmIhkBAACmIhkBAACmIhnBLWHRokUKDAw0OwzAIRaLRatXrzY7DMB0JCNwKcePH9cTTzyh0NBQeXl5KTw8XM8++6zOnDlja1O7dm29/vrr5gUJFENKSoqeeeYZ1alTR1arVWFhYerVq5c2bdpkdmiSLj9VdMKECQoJCZGPj4+io6N14MABs8OCmyIZgcs4dOiQWrdurQMHDui9997TwYMHNX/+fG3atElRUVFKTU296THl5ube9Gui7Dty5IhatWqlzZs3a8aMGdq9e7fWrVunjh07Kj4+3uzwJEnTp0/X7NmzNX/+fG3btk2+vr6KiYlRVlaW2aHBHRmAi+jatatRs2ZN48KFC3b7T5w4YVSoUMF48sknjfbt2xuS7DbDMIyFCxcaAQEBxrp164xGjRoZvr6+RkxMjJGcnGzX14IFC4xGjRoZVqvVaNiwoTF37lzbscOHDxuSjBUrVhjt2rUzrFarsXDhwlK/b9x6unXrZtSoUcPIyMgodOzs2bO2nyUZq1atsn0eO3asUb9+fcPHx8eIiIgwXnzxRSMnJ8d2fNeuXUaHDh2MihUrGn5+fsbtt99ubN++3TAMwzhy5IjRs2dPIzAw0KhQoYLRuHFj45NPPikyvvz8fCM4ONiYMWOGbV9aWpphtVqN9957r4R3DziOd9PAJaSmpmr9+vWaOnWqfHx87I4FBwcrNjZWK1eu1IEDB9SiRQsNGTJEgwcPtmt34cIFzZw5U0uXLpWHh4ceeeQRjR49WsuWLZMkLVu2TBMmTNCbb76pli1baufOnRo8eLB8fX0VFxdn6+e5557Tq6++qpYtW7r1uyJwY1JTU7Vu3TpNnTpVvr6+hY5fa26Tn5+fFi1apNDQUO3evVuDBw+Wn5+fxo4dK0mKjY1Vy5YtNW/ePHl6emrXrl0qX768JCk+Pl45OTlKSEiQr6+v9u7dq4oVKxZ5ncOHDyslJUXR0dG2fQEBAWrTpo0SExPVv3//EnwDgONIRuASDhw4IMMwFBkZWeTxyMhInT17Vnl5efL09JSfn5+Cg4Pt2uTm5mr+/PmqW7euJGno0KGaMmWK7fjEiRP16quvqm/fvpKkiIgI7d27V2+//bZdMjJ8+HBbG8BRBw8elGEYatSokcPnvvjii7afa9eurdGjR2vFihW2ZOTYsWMaM2aMre/69evb2h87dkz9+vVT06ZNJUl16tS56nVSUlIkSUFBQXb7g4KCbMeAm4lkBC7FKMEDgStUqGBLRCQpJCREp06dkiRlZmYqKSlJAwcOtKuoXLp0qdDbOFu3bn3DMQAl+TO8cuVKzZ49W0lJScrIyNClS5fs3sw7cuRIDRo0SEuXLlV0dLTuv/9+25/5YcOG6amnntKGDRsUHR2tfv36qVmzZiW+H+BmYAIrXEK9evVksVi0b9++Io/v27dPlSpVUrVq1a7aR0G5uoDFYrH9YsjIyJAkLViwQLt27bJtP//8s7799lu784oqrQPFVb9+fVksFv3yyy8OnZeYmKjY2Fh1795da9eu1c6dO/XCCy8oJyfH1mbSpEnas2ePevTooc2bN6tx48ZatWqVJGnQoEE6dOiQHn30Ue3evVutW7fWnDlzirxWQVXx5MmTdvtPnjxZqOII3AwkI3AJVapUUefOnfXWW2/p4sWLdsdSUlK0bNkyPfjgg7JYLPLy8lJeXp5D/QcFBSk0NFSHDh1SvXr17LaIiAhn3grcXOXKlRUTE6O5c+cqMzOz0PG0tLQiz9u6davCw8P1wgsvqHXr1qpfv76OHj1aqF2DBg00YsQIbdiwQX379tXChQttx8LCwvTkk0/qo48+0qhRo7RgwYIirxUREaHg4GC7Zcbp6enatm2boqKiHLxjoORIRuAy3nzzTWVnZysmJkYJCQk6fvy41q1bp86dO6tGjRqaOnWqpMtj6QkJCfr999/1xx9/FLv/yZMna9q0aZo9e7Z+/fVX7d69WwsXLtRrr71WWrcENzV37lzl5eXpL3/5iz788EMdOHBA+/bt0+zZs6/6y75+/fo6duyYVqxYoaSkJM2ePdtW9ZCkixcvaujQofryyy919OhRffPNN9q+fbttntXw4cO1fv16HT58WD/88IO++OKLq87BslgsGj58uF5++WX95z//0e7du/XYY48pNDRUffr0cfr3AVyXqWt5gCscOXLEiIuLM4KCgozy5csbYWFhxjPPPGP88ccftjaJiYlGs2bNDKvVWmhp75+tWrXKuPKP+LJly4wWLVoYXl5eRqVKlYx27doZH330kWEY/13au3PnzlK9R7iH5ORkIz4+3ggPDze8vLyMGjVqGPfcc4/xxRdf2NroiqW9Y8aMMapUqWJUrFjRePDBB41Zs2bZ/lxnZ2cb/fv3N8LCwgwvLy8jNDTUGDp0qHHx4kXDMAxj6NChRt26dQ2r1WpUq1bNePTRR+3+3lwpPz/fGD9+vBEUFGRYrVajU6dOxv79+0vjqwCuy2IYJZhtBQAAUEIM0wAAAFORjAAAAFORjAAAAFORjAAAAFORjAAAAFORjAAAAFORjAAAAFORjABuYMCAAXZP1uzQoYOGDx9+0+P48ssvZbFYrvpIdOny00FXr15d7D4nTZqkFi1alCiuI0eOyGKxaNeuXSXqB8CNIRkBTDJgwABZLBbb+3bq1aunKVOm6NKlS6V+7Y8++kgvvfRSsdoWJ4EAgJIoZ3YAgDvr2rWrFi5cqOzsbH366aeKj49X+fLl9fzzzxdqm5OTIy8vL6dct3Llyk7pBwCcgcoIYCKr1arg4GCFh4frqaeeUnR0tP7zn/9I+u/QytSpUxUaGqqGDRtKko4fP64HHnhAgYGBqly5snr37q0jR47Y+szLy9PIkSMVGBioKlWqaOzYsbryrQ9XDtNkZ2dr3LhxCgsLk9VqVb169fTuu+/qyJEj6tixoySpUqVKslgsGjBggCQpPz9f06ZNU0REhHx8fNS8eXP9+9//trvOp59+qgYNGsjHx0cdO3a0i7O4xo0bpwYNGqhChQqqU6eOxo8fr9zc3ELt3n77bYWFhalChQp64IEHdO7cObvj77zzjiIjI+Xt7a1GjRrprbfecjgWAKWDZARwIT4+PsrJybF93rRpk/bv36+NGzdq7dq1ys3NVUxMjPz8/PTVV1/pm2++UcWKFdW1a1fbea+++qoWLVqk//u//9PXX3+t1NRUu7e/FuWxxx7Te++9p9mzZ2vfvn16++23VbFiRYWFhenDDz+UJO3fv18nTpzQG2+8IUmaNm2alixZovnz52vPnj0aMWKEHnnkEW3ZskXS5aSpb9++6tWrl3bt2qVBgwbpueeec/g78fPz06JFi7R371698cYbWrBggWbNmmXX5uDBg3r//fe1Zs0arVu3Tjt37tTTTz9tO75s2TJNmDBBU6dO1b59+/TKK69o/PjxWrx4scPxACgFJr+oD3BbcXFxRu/evQ3DuPwG1Y0bNxpWq9UYPXq07XhQUJCRnZ1tO2fp0qVGw4YNjfz8fNu+7Oxsw8fHx1i/fr1hGIYREhJiTJ8+3XY8NzfXqFmzpu1ahmEY7du3N5599lnDMAxj//79hiRj48aNRcb5xRdfGJKMs2fP2vZlZWUZFSpUMLZu3WrXduDAgcZDDz1kGIZhPP/880bjxo3tjo8bN65QX1fSFW+yvdKMGTOMVq1a2T5PnDjR8PT0NH777Tfbvs8++8zw8PAwTpw4YRiGYdStW9dYvny5XT8vvfSSERUVZRgGb2wGzMacEcBEa9euVcWKFZWbm6v8/Hw9/PDDmjRpku1406ZN7eaJ/Pjjjzp48KD8/Pzs+snKylJSUpLOnTunEydOqE2bNrZj5cqVU+vWrQsN1RTYtWuXPD091b59+2LHffDgQV24cEGdO3e225+Tk6OWLVtKkvbt22cXhyRFRUUV+xoFVq5cqdmzZyspKUkZGRm6dOmS/P397drUqlVLNWrUsLtOfn6+9u/fLz8/PyUlJWngwIEaPHiwrc2lS5cUEBDgcDwAnI9kBDBRx44dNW/ePHl5eSk0NFTlytn/lfT19bX7nJGRoVatWmnZsmWF+qpWrdoNxeDj4+PwORkZGZKkTz75xC4JkC7Pg3GWxMRExcbGavLkyYqJiVFAQIBWrFihV1991eFYFyxYUCg58vT0dFqsAG4cyQhgIl9fX9WrV6/Y7W+//XatXLlS1atXL1QdKBASEqJt27apXbt2ki5XAL7//nvdfvvtRbZv2rSp8vPztWXLFkVHRxc6XlCZycvLs+1r3LixrFarjh07dtWKSmRkpG0yboFvv/32+jf5J1u3blV4eLheeOEF276jR48Wanfs2DElJycrNDTUdh0PDw81bNhQQUFBCg0N1aFDhxQbG+vQ9QHcHExgBcqQ2NhYVa1aVb1799ZXX32lw4cP68svv9SwYcP022+/SZKeffZZ/eMf/9Dq1av1yy+/6Omnn77mM0Jq166tuLg4PfHEE1q9erWtz/fff1+SFB4eLovForVr1+r06dPKyMiQn5+fRo8erREjRmjx4sVKSkrSDz/8oDlz5tgmhT755JM6cOCAxowZo/3792v58uVatGiRQ/dbv359HTt2TCtWrFBSUpJmz55d5GRcb29vxcXF6ccff9RXX32lYcOG6YEHHlBwcLAkafLkyZo2bZpmz56tX3/9Vbt379bChQv12muvORQPgNJBMgKUIRUqVFBCQoJq1aqlvn37KjIyUgMHDlRWVpatUjJq1Cg9+uijiouLU1RUlPz8/HTvvfdes9958+bpvvvu09NPP61GjRpp8ODByszMlCTVqFFDkydP1nPPPaegoCANHTpUkvTSSy9p/PjxmjZtmiIjI9W1a1d98sknioiIkHR5HseHH36o1atXq3nz5po/f75eeeUVh+73nnvu0YgRIzR06FC1aNFCW7du1fjx4wu1q1evnvr27avu3burS5cuatasmd3S3UGDBumdd97RwoUL1bRpU7Vv316LFi2yxQrAXBbjarPaAAAAbgIqIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFQkIwAAwFT/D5Z8HU+XPOybAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(classification_report(y_te, clf.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kDC66azLL7w",
        "outputId": "ccb85a74-9dfc-4aa2-eaca-f8c474f7a060"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98       123\n",
            "           1       0.99      0.99      0.99       126\n",
            "           2       0.95      0.99      0.97       126\n",
            "\n",
            "    accuracy                           0.98       375\n",
            "   macro avg       0.98      0.98      0.98       375\n",
            "weighted avg       0.98      0.98      0.98       375\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")   # should contain ‘Survived’ as target\n",
        "y = df[\"Survived\"]\n",
        "X = df.drop(columns=[\"Survived\", \"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],errors=\"ignore\")\n",
        "\n",
        "num_cols = X.select_dtypes(include=\"number\").columns\n",
        "cat_cols = X.select_dtypes(include=\"object\").columns\n",
        "\n",
        "prep = ColumnTransformer([\n",
        "    (\"num\", SimpleImputer(strategy=\"mean\"), num_cols), # Add imputer for numerical columns\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "])\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"prep\", prep),\n",
        "    (\"clf\",  LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "pipe.fit(X_tr, y_tr)\n",
        "print(\"Titanic accuracy:\", accuracy_score(y_te, pipe.predict(X_te)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-8Bo3FqLacB",
        "outputId": "af3b9cd8-93ca-4365-f12a-f07a5b4d8261"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic accuracy: 0.8044692737430168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "plain = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "plain.fit(X_tr, y_tr)\n",
        "acc_plain = accuracy_score(y_te, plain.predict(X_te))\n",
        "\n",
        "# With scaling\n",
        "scaled = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\",    LogisticRegression(max_iter=1000, multi_class=\"multinomial\"))\n",
        "])\n",
        "scaled.fit(X_tr, y_tr)\n",
        "acc_scaled = accuracy_score(y_te, scaled.predict(X_te))\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_plain:.5f}\")\n",
        "print(f\"Accuracy with scaling   : {acc_scaled:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK4f2KrEPqVU",
        "outputId": "e5b31d53-2f60-418b-c9dd-f7bb519aa90a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.98000\n",
            "Accuracy with scaling   : 0.98000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), (df[\"target\"] == 0).astype(int)  # binary\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "probs = clf.predict_proba(X_te)[:, 1]\n",
        "print(\"ROC‑AUC:\", roc_auc_score(y_te, probs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkaToq67QQJQ",
        "outputId": "f559c113-b11b-4140-9a6b-5d360bab9604"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC‑AUC: 0.9978706929926442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(C=0.5, max_iter=1000, multi_class=\"multinomial\")\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Accuracy (C=0.5):\", accuracy_score(y_te, clf.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95MB2MQSQag7",
        "outputId": "5895e486-2aa4-46a0-9d1e-105531ec96e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (C=0.5): 0.9813333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "# Absolute magnitude → importance\n",
        "coefs = pd.Series(clf.coef_.ravel(), index=X.columns.repeat(len(clf.classes_)))\n",
        "top = coefs.abs().sort_values(ascending=False).head(10)\n",
        "print(\"Top‑10 important features:\")\n",
        "print(top)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Qx6YCJRLTf",
        "outputId": "655890da-5045-47a2-dee5-8a408d2ca142"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top‑10 important features:\n",
            "feat_16    0.372802\n",
            "feat_2     0.366686\n",
            "feat_18    0.359646\n",
            "feat_4     0.344633\n",
            "feat_10    0.301748\n",
            "feat_8     0.290849\n",
            "feat_17    0.280559\n",
            "feat_13    0.272735\n",
            "feat_5     0.259410\n",
            "feat_17    0.235349\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Cohen’s kappa:\", round(cohen_kappa_score(y_te, clf.predict(X_te)), 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwvCJwWzRlp3",
        "outputId": "ac20e8ab-3bd4-42a3-c324-862a770afce8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen’s kappa: 0.972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X = df.drop(columns=\"target\")\n",
        "y = (df[\"target\"] == 0).astype(int)          # binary: “class 0” vs “others”\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "probs = clf.predict_proba(X_te)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_te, probs)\n",
        "ap = average_precision_score(y_te, probs)\n",
        "\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision–Recall Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iF2oWnJxRyHq",
        "outputId": "124e103c-42e3-420b-abf2-213ca0f3732e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "solvers = [\"liblinear\", \"saga\", \"lbfgs\"]\n",
        "for s in solvers:\n",
        "    clf = LogisticRegression(solver=s, max_iter=5000,\n",
        "                             multi_class=\"multinomial\" if s != \"liblinear\" else \"ovr\")\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    acc = accuracy_score(y_te, clf.predict(X_te))\n",
        "    print(f\"{s:<9} accuracy: {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_u93MzySdDq",
        "outputId": "bbf82085-4274-438c-8dbc-4b7e5a2e6f47"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liblinear accuracy: 0.981\n",
            "saga      accuracy: 0.981\n",
            "lbfgs     accuracy: 0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), (df[\"target\"] == 0).astype(int)  # binary\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"MCC:\", round(matthews_corrcoef(y_te, clf.predict(X_te)), 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udwi5cOWSs0a",
        "outputId": "41d14f7c-65cb-4a74-cdbc-1f027001c00a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "plain = LogisticRegression(max_iter=1000, multi_class=\"multinomial\").fit(X_tr, y_tr)\n",
        "acc_plain = accuracy_score(y_te, plain.predict(X_te))\n",
        "\n",
        "scaled = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\",    LogisticRegression(max_iter=1000, multi_class=\"multinomial\"))\n",
        "]).fit(X_tr, y_tr)\n",
        "acc_scaled = accuracy_score(y_te, scaled.predict(X_te))\n",
        "\n",
        "print(f\"Raw accuracy       : {acc_plain:.3f}\")\n",
        "print(f\"Standardized accuracy: {acc_scaled:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrHAPOJZS3R5",
        "outputId": "242c2e8c-772c-4393-f07f-c2ad2f80859d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw accuracy       : 0.980\n",
            "Standardized accuracy: 0.980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "Cs = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=5000, multi_class=\"multinomial\"), {\"C\": Cs}, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Best C:\", grid.best_params_[\"C\"])\n",
        "print(\"Best CV accuracy:\", round(grid.best_score_, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrFn_ZkHTa0J",
        "outputId": "86b1218b-a2af-4cf6-b2b2-6417bf242b51"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 0.1\n",
            "Best CV accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "X, y = df.drop(columns=\"target\"), df[\"target\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "model.fit(X_tr, y_tr)\n",
        "\n",
        "joblib.dump(model, \"logreg_model.joblib\")      # save\n",
        "loaded = joblib.load(\"logreg_model.joblib\")    # reload\n",
        "\n",
        "print(\"Reloaded model accuracy:\",accuracy_score(y_te, loaded.predict(X_te)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH3XNNByTp7I",
        "outputId": "8cdf8aa8-8550-4dc6-a717-9df57091e53c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloaded model accuracy: 0.9813333333333333\n"
          ]
        }
      ]
    }
  ]
}